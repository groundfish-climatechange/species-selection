---
title: "Prepare Observer Data"
author: "Owen Liu"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(here)
library(magrittr)
```

# Purpose

Clean, explore, and prepare the WGOP (observer) data for cluster analyses.

# Import

Import the data, obtained from Kate Richerson, May, 2021.

The data came with a description file, describing the variables in the data:

```{r}
info <- read_csv(here('data','wcgop_obproc_legend_20210430.csv'),col_types = 'cc')
knitr::kable(info)
```

And the observer data itself. It's a lot of data! But we can probably filter out a lot of variables that we do not need.

```{r}
dat <- read_csv(here('data','wcgop_obproc_20210430.csv'),
                # we use the table above to choose only some columns to read
                # can edit this later if we want to look at other variables
                # we also use this call to read_csv() to indicate the column data types,
                # to avoid parsing errors
                col_types = cols_only(
                  AREA = "c",
                  AVG_DEPTH = "d",
                  AVG_LONG = 'd',
                  AVG_LAT = 'd',
                  CATCH_DISPOSITION = 'c',
                  DIS_MT = 'd',
                  DRVID = 'c',
                  VESSEL = 'c',
                  GEAR_TYPE = 'c',
                  GF = 'l',
                  GROUPING = 'c',
                  hake_sector = 'c',
                  HAUL_ID = 'i',
                  IFQ = 'l',
                  IFQ_GROUPING = 'c',
                  RET_MT = 'd',
                  scientific_name='c',
                  sector = 'c',
                  SPECIES_CATEGORY_NAME= 'c',
                  species='c',
                  SET_DATE = col_datetime(format = ""),
                  TARGET = 'c',
                  TRIP_ID = 'i',
                  MT = 'd'
                ))
```

# Clean Data

Here, we remove empty or unidentified records. We'll keep track of how many records are removed.

* Remove records with no recorded catch weight
* Remove records with no recorded set date, latitude, or longitude
* Remove records with no recorded haul identification number.

The raw dataset after importing from `csv`, as analyzed on `r format(Sys.time(), '%d %B, %Y')`, has `r nrow(dat)` observations.

## No Catch Weight

```{r}
dat_f1 <- dat %>% 
  filter(!is.na(MT))
```

Filtering out records with no recorded catch weight (`MT`) removes `r nrow(dat)-nrow(dat_f1)` observations, or `r (nrow(dat)-nrow(dat_f1))/nrow(dat)*100` percent.

## No Date, Latitude, or Longitude

We filter out any records that are missing date, lat, or lon identifiers.

```{r}
dat_f2 <- dat_f1 %>% 
  filter(!is.na(SET_DATE),
         !is.na(AVG_LONG),
         !is.na(AVG_LAT))
```

Filtering out these records removes another `r nrow(dat_f1)-nrow(dat_f2)` observations, or `r (nrow(dat_f1)-nrow(dat_f2))/nrow(dat)*100` percent of the original data.

## No Haul ID

We filter out any records that are missing a unique haul identification number.

```{r}
dat_f3 <- dat_f2 %>% 
  filter(!is.na(HAUL_ID))
```

The "thinned" data now look like this:

```{r}
glimpse(dat_f3)
```


Filtering out these records removes another `r nrow(dat_f2)-nrow(dat_f3)` observations, or `r (nrow(dat_f2)-nrow(dat_f3))/nrow(dat)*100` percent of the original data. It seems that many of the observations that were missing one or more of the above variables (e.g., date) were likely to be missing others as well.

```{r,echo=F,include=F}
rm(dat,dat_f1,dat_f2)
```

# Prepare Data for Cluster Analysis

Finally, we want to organize the data such that it is prepared for cluster analysis. To do this, we alter the format from "long" to "wide" format, such that each row should have a unique haul ID, and the columns should indicate the catch of each species in that haul.

For now, we only retain a small number of haul-level identifiers (Haul ID, date, lat/lon of set, gear type, sector). There should be `r length(unique(dat_f3$HAUL_ID))` unique hauls.

Later, we can pull other variables from the observer data if they become important.

## Species Exclusion Criteria

We do not want to remove too many species now (after all, the point of this analysis is to figure out which species are worth focusing on), but there are `r length(unique(dat_f3$species))` unique species in this dataset. Here, we subset just to species in the Groundfish FMP, then look at each species' percent contribution to the TOTAL landed weight across all observations.


### Groundfish FMP

For the time being, select only species that appear in the groundfish Fishery Management Plan

```{r}
dat_gf <- dat_f3 %>% 
  filter(GF)
```


### Non-species

Some of these are not species at all, but other categories (unidentified fish, rocks and mud, etc.). Let's filter these out. First, remove any species with "Unid" in the name, indicating unidentified species of some sort.

```{r}
dat_gf %<>% 
  filter(!grepl("Unid",species))
```

After these filters, there are `r length(unique(dat_gf$species))` left in the dataset.

### Catch Rank

```{r}
spp_ranks <- dat_gf %>% 
  # for now we group by common name- need to check the intricacies of the "SPID_EQV" identifier
  group_by(species) %>% 
  summarise(total_MT_thousands=sum(MT)/1e3) %>% 
  ungroup() %>% 
  mutate(prop_weight=total_MT_thousands/sum(total_MT_thousands)) %>% 
  mutate(rnk=min_rank(desc(prop_weight)))
```

```{r,include=F}
# species name-matching key
# should come back to check this- it is a many-to-many match
# spp_names<-dat_gf %>% 
#   distinct(SPID_EQV,species)
```

The top 25 species by weight in the entire dataset:

```{r}
spp_ranks %>% 
  slice_max(order_by=desc(rnk),n=25) %>% 
  arrange(rnk) %>% 
  ggplot(aes(x=fct_reorder(species,rnk),y=log(total_MT_thousands)))+
  geom_col()+
  # scale_y_continuous(breaks=seq(0,10,by=2))+
  labs(x="Species",y="Log Total Catch (Thousand MT)",title="Total Haul-level Catch Recorded in the Observer Data")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,vjust=1.2,hjust=1,size=8),
        panel.grid.major = element_blank(),
        panel.grid.minor=element_blank(),
        axis.title.x = element_text(size=10),
        axis.title.y=element_text(size=10))
```

And if we remove Pacific hake (whiting), as by far the most-represented species:

```{r}
spp_ranks %>% 
  filter(rnk != 1) %>% 
  slice_max(order_by=desc(rnk),n=25) %>% 
  arrange(rnk) %>% 
  ggplot(aes(x=fct_reorder(species,rnk),y=log(total_MT_thousands)))+
  geom_col()+
  # scale_y_continuous(breaks=seq(0,10,by=2))+
  labs(x="Species",y="Log Total Catch (Thousand MT)",
       title="Total Haul-level Catch Recorded in the Observer Data\n(Non-whiting)")+
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle=45,vjust=1.2,hjust=1,size=8),
        panel.grid.major = element_blank(),
        panel.grid.minor=element_blank(),
        axis.title.x = element_text(size=10),
        axis.title.y=element_text(size=10))
```

How many species do we cut out if we make a cutoff at 1% of all landings? 0.1%?

```{r}
spp_one_percent <- spp_ranks %>% 
  filter(prop_weight > 0.01) %>% 
  distinct(species)

spp_point1_percent <- spp_ranks %>% 
  filter(prop_weight > 0.001) %>% 
  distinct(species)
```

Turns out, making a cutoff at 1% of total, whole-dataset landings only leaves us with `r nrow(spp_one_percent)` species:

```{r}
spp_one_percent$species
```

If we make the cutoff 0.1%, there are `r nrow(spp_point1_percent)` species:

```{r}
spp_point1_percent$species
```

As a final exploration (for now), what about any species that represents greater than 1% by landed weight of *any recorded haul* (not just across the entire dataset)?

```{r}
spp_ranks_haul <- dat_gf %>% 
  # for now we group by common name- need to check the intricacies of the "SPID_EQV" identifier
  group_by(HAUL_ID,species) %>% 
  summarise(total_MT=sum(MT)) %>% 
  ungroup() %>% 
  group_by(HAUL_ID) %>% 
  mutate(prop_weight_haul=total_MT/sum(total_MT)) %>% 
  ungroup()
```

```{r}
spp_haul_one_percent <- spp_ranks_haul %>% 
  filter(prop_weight_haul > 0.01) %>% 
  distinct(species)
```

This cutoff gives us `r nrow(spp_haul_one_percent)` species.

## Cast Data to Wide Form

I'm going to use (for now) the version of the data with the 0.1% cutoff.

For this version of the data, in order to summarize across species, we need to remove variables that categorize the catches into smaller categories than the haul-level (for example, discarded vs. retained catch of the same species)

```{r}
dat_wide <- dat_gf %>%
  filter(species %in% spp_point1_percent$species) %>% 
  # first, select only a subset of variables
  dplyr::select(-DIS_MT,-RET_MT,-GF) %>%
  group_by(HAUL_ID,sector,hake_sector,DRVID,VESSEL,TRIP_ID,AREA,AVG_LAT,AVG_LONG,AVG_DEPTH,
           GEAR_TYPE,TARGET,SET_DATE,species) %>% 
  # sum all catch per haul by species
  summarise(MT=sum(MT)) %>% 
  # then, cast to wide-form
  group_by(HAUL_ID,sector,hake_sector,DRVID,VESSEL,TRIP_ID,AREA,AVG_LAT,AVG_LONG,AVG_DEPTH,
           GEAR_TYPE,TARGET,SET_DATE) %>% 
  arrange(species) %>% 
  pivot_wider(names_from = "species",values_from="MT") %>% 
  ungroup()
```

```{r}
# Fill in zeroes for no catch
dat_wide %<>%
  mutate(across(`Arrowtooth Flounder`:`Yellowtail Rockfish`,~replace(., is.na(.), 0)))
```

This gives us a unique row/observation for each separate `HAUL_ID`, with columns for the catch of each species.

# Save

Save this clean, thinned, filtered, and wide-form version of the data.

```{r}
write_rds(dat_wide,here('data','wcgop_wide.rds'))
```

And the final data ready for clustering looks something like this:

```{r}
glimpse(dat_wide)
```


